services:
  # Main API service
  api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: job-scraper-api
    environment:
      - ENVIRONMENT=development
      - DATABASE_URL=postgresql://postgres:postgres@postgres:5432/jobscraper
      - REDIS_URL=redis://redis:6379/0
      - KAFKA_BOOTSTRAP_SERVERS=kafka:29092
    ports:
      - "8000:8000"
      - "9090:9090"
    volumes:
      - ./src:/app/src
      - ./logs:/app/logs
      - ./data:/app/data
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      kafka:
        condition: service_healthy
    networks:
      - job-scraper-net
    restart: unless-stopped

  # Scrapy workers
  scrapy-worker:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: job-scraper-scrapy
    command: ["python", "-m", "scrapy", "crawl", "job_spider"]
    environment:
      - ENVIRONMENT=development
      - DATABASE_URL=postgresql://postgres:postgres@postgres:5432/jobscraper
      - REDIS_URL=redis://redis:6379/0
      - KAFKA_BOOTSTRAP_SERVERS=kafka:29092
    volumes:
      - ./src:/app/src
      - ./logs:/app/logs
    depends_on:
      - api
      - postgres
      - redis
      - kafka
    networks:
      - job-scraper-net
    restart: unless-stopped

  # PostgreSQL with pgvector
  postgres:
    image: ankane/pgvector:latest
    container_name: job-scraper-postgres
    environment:
      - POSTGRES_USER=postgres
      - POSTGRES_PASSWORD=postgres
      - POSTGRES_DB=jobscraper
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./infrastructure/docker/postgres/init.sql:/docker-entrypoint-initdb.d/init.sql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U postgres"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - job-scraper-net
    restart: unless-stopped

  # Redis Cluster
  redis:
    image: redis:7-alpine
    container_name: job-scraper-redis
    command: redis-server --appendonly yes --maxmemory 512mb --maxmemory-policy allkeys-lru
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - job-scraper-net
    restart: unless-stopped

  # Kafka
  zookeeper:
    image: confluentinc/cp-zookeeper:latest
    container_name: job-scraper-zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    networks:
      - job-scraper-net
    restart: unless-stopped

  kafka:
    image: confluentinc/cp-kafka:latest
    container_name: job-scraper-kafka
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
    depends_on:
      - zookeeper
    healthcheck:
      test: ["CMD", "kafka-broker-api-versions", "--bootstrap-server", "localhost:9092"]
      interval: 30s
      timeout: 10s
      retries: 5
    networks:
      - job-scraper-net
    restart: unless-stopped

  # Prometheus monitoring
  prometheus:
    image: prom/prometheus:latest
    container_name: job-scraper-prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
    ports:
      - "9091:9090"
    volumes:
      - ./infrastructure/monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    networks:
      - job-scraper-net
    restart: unless-stopped

  # Grafana dashboards
  grafana:
    image: grafana/grafana:latest
    container_name: job-scraper-grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - grafana_data:/var/lib/grafana
      - ./infrastructure/monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./infrastructure/monitoring/grafana/datasources:/etc/grafana/provisioning/datasources
    depends_on:
      - prometheus
    networks:
      - job-scraper-net
    restart: unless-stopped

  # Jaeger tracing
  jaeger:
    image: jaegertracing/all-in-one:latest
    container_name: job-scraper-jaeger
    ports:
      - "5775:5775/udp"
      - "6831:6831/udp"
      - "6832:6832/udp"
      - "5778:5778"
      - "16686:16686"
      - "14268:14268"
      - "14250:14250"
      - "9411:9411"
    environment:
      - COLLECTOR_ZIPKIN_HOST_PORT=:9411
    networks:
      - job-scraper-net
    restart: unless-stopped

  # Selenium Grid Hub (for distributed browser automation)
  selenium-hub:
    image: selenium/hub:latest
    container_name: job-scraper-selenium-hub
    ports:
      - "4444:4444"
    networks:
      - job-scraper-net
    restart: unless-stopped

  # Chrome nodes for Selenium
  chrome-node:
    image: selenium/node-chrome:latest
    container_name: job-scraper-chrome-node
    environment:
      - SE_EVENT_BUS_HOST=selenium-hub
      - SE_EVENT_BUS_PUBLISH_PORT=4442
      - SE_EVENT_BUS_SUBSCRIBE_PORT=4443
      - SE_NODE_MAX_SESSIONS=5
    depends_on:
      - selenium-hub
    networks:
      - job-scraper-net
    restart: unless-stopped

volumes:
  postgres_data:
  redis_data:
  prometheus_data:
  grafana_data:

networks:
  job-scraper-net:
    driver: bridge
